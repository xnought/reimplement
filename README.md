# reimplement

Goal: learn Jax and distributed training/inference by the end of January (in roughly 5 weeks).

**Reimplement the following models/papers:**

- [ ] CNN with MNIST (week 1)
- [ ] CNN Imagenet (week 2)
- [ ] CNN variable computation test (week 3)
- [ ] GPT2 / NanoGPT (week 3)
- [ ] GPT3 (week 4)
- [ ] GPT with test time RL training (week 5)
- [ ] SAE features (week 5)

For these to be any good, especially the LLMs, I'll need to learn how to run on distributed hardware. Let's do it!

All the computation will be in notebooks for the given task for ease of use and visualization.
